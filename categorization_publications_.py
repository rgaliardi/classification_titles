# -*- coding: utf-8 -*-
"""Categorization-Publications.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10UrAioCxS8XptMujTsEVsT8CMuhi0cbO

## 1. Importação de Biblotecas de Apoio
"""

# !pip install --upgrade pip --user
# !pip install tensorflow --user
# !pip install tensorflow-gpu --user
# !pip install keras --user
# !pip install --user -U nltk
# !pip install tensorflow-gpu --user

# !conda create --name PythonGPU
# !activate PythonCPU
# !conda install -c anaconda keras
# !conda install -c anaconda keras-gpu

# !conda install -c theano
# !conda install -c conda-forge keras tensorflow

"""## 2. Preparação do Ambiente

### 2.1 - Importação das Biblotecas Base
"""

# Carrega as bibliotecas de ambiente

import os
import io
import gc
import re
import string
import requests
import collections

path = os.getcwd()

"""### 2.2 - Importação das Biblotecas Específicas"""

# Commented out IPython magic to ensure Python compatibility.
# Carrega as bibliotecas de ciências e gráficos

import pickle

import theano
import nltk
import keras
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

from numba import vectorize
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from sklearn import preprocessing
from sklearn import metrics

warnings.filterwarnings('ignore')
plt.switch_backend('agg')
# %matplotlib inline

gc.get_threshold()

"""### 2.3 - Importação de Dados de Pacotes"""

# Importação das stopwords do pacote nltk

nltk.download('stopwords')

"""### 2.4 - Definição das Constantes de Configuração"""

# Caminho para os arquivos de dados
PATH = "input/"

# Regex
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
ONLY_STRING_WORD_RE = re.compile(r'\w*\d\w*')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
LESS_THAN_WORD_RE = re.compile(r'\b\w{1,2,3}\b')
REMOVE_NUMBERS_RE = re.compile(' \d+')

# Stopwords
STOPWORDS_0 = set(stopwords.words('english'))
STOPWORDS_1 = set(stopwords.words('portuguese'))
STOPWORDS_2 = set(stopwords.words('spanish'))

# Número de Palavras por Indice
WORD_INDEX = 0

# Número máximo de palavras usadas mais frequentes
MAX_NB_WORDS = 50000
# Numero máximo de palavras para saída
MAX_SEQUENCE_LENGTH = 100
# Fixador.
EMBEDDING_DIM = 100
# Variável randomica
RANDOM_STATE = 2011

# Número de épocas
EPOCHS = 15
# Tamanho do bloco
BATCH_SIZE = 64

"""## 3. Funções de Apoio"""

# Cria ou lê os dados pickle
def file_pickle(file):
    fpkl = PATH + file + ".pickle"
    fcsv = PATH + file + ".csv"
  
    if os.path.isfile(fpkl):
        df = pd.read_pickle(fpkl)
    else:        
        df = pd.read_csv(fcsv, header=0, sep=',', quotechar='"', error_bad_lines=False, skipinitialspace=True)
        df.to_pickle(fpkl)

    return df

# Atualiza o arquivo pickle com novas informações
def update_pickle(file, df):
    fpkl = PATH + file + ".pkl"
    df.to_pickle(fpkl)
    
# Imprime os dados relativos ao indice passado
def print_plot(index):
    example = dftrain[dftrain.index == index][['title', 'category']].values[0]
    
    if len(example) > 0:
        print(example[0])
        print('Category:', example[1])
        
# Limpeza dos dados: lower case; espaços do texto; caracteres especiais e simbolos; stop words e digitos
def clean_text(text):
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = LESS_THAN_WORD_RE.sub('', text) # replace LESS_THAN_WORD symbols by space in text. substitute the matched string in LESS_THAN_WORD with space.
    text = ONLY_STRING_WORD_RE.sub('', text) # replace ONLY_STRING_WORD_RE REMOVE words with numbers and letters in text. substitute the matched string in ONLY_STRING_WORD_RE with space.    
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = REMOVE_NUMBERS_RE.sub('', text) # remove numbers which are in REMOVE_NUMBERS from text. substitute the matched string in REMOVE_NUMBERS with nothing. 

    #    text = re.sub(r'\W+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS_0) # remove stopwors english from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS_1) # remove stopwors portugues from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS_2) # remove stopwors spanish from text
    text = text.replace('\d+', '')
        
    return text

# Tokenização de textos
def token_text(text):
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters=string.punctuation, lower=True)
    tokenizer.fit_on_texts(text)
    WORD_INDEX = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(text)
    sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    return sequences

# Conversão de texto para variável categórica
def dummie_text(text):
    dummies = pd.get_dummies(text).values
    
    return dummies
  
# Função principal de limpeza e tokenização (wording embedding)
def converter_text(row):
  title = clean_text(row["title"])
  row["title_"] = token_text(title)
  row["language_"] = dummie_text(row["language"])
  
  if dftest.columns.isin(['category']).any():
    row["category_"] = dummie_text(row["category"])
  
  return row

"""## 4. Coleta de Dados

### 4.1 - Carga dos Dados de Treino
"""

# urltrain = "https://meli-data-challenge.s3.amazonaws.com/train.csv.gz"
# ctrain = requests.get(urltrain).content
# ftrain = pd.read_csv(io.StringIO(ctrain.decode('utf-8')), compression='gzip', header=0, sep=',', quotechar='"', error_bad_lines=False, skipinitialspace=True)
# dftrain = pd.read_csv("input/train.csv.gz", compression='gzip', header=0, sep=',', quotechar='"', error_bad_lines=False, skipinitialspace=True)

dftrain = file_pickle("train")

"""### 4.2 - Carga dos Dados de Teste"""

# urltest = "https://meli-data-challenge.s3.amazonaws.com/test.csv"
# ctest = requests.get(urltest).content
# ftest = pd.read_csv(io.StringIO(ctest.decode('utf-8')), header=0, sep=',', quotechar='"', error_bad_lines=False, skipinitialspace=True)
# dftest = pd.read_csv("input/test.csv", header=0, sep=',', quotechar='"', error_bad_lines=False, skipinitialspace=True)

dftest = file_pickle("test")

"""### 4.3 - Corrige as Variáveis de configuração"""

MAX_SEQUENCE_LENGTH = dftest["title"].map(len).max()

gc.collect()
gc.get_threshold()

"""## 5. Processamento/Tratamento de Dados

### 5.1 - Análise dos dados de treino
"""

# Verifica a estrutura básica dos dados de treino

print('Shape of dataset ',dftrain.shape)
print(dftrain.columns)

dftrain.head(5)

# Verificação das caracteristicas de cada coluna do arquivo

dftrain.info()

# Verifica se exitem dados nulos no geral

if dftrain.isnull().values.any():
    dftrain[dftrain.isnull().any(axis=1)] 
else:
    print("Nos dados de treino não existem dados nulos.")

# Remove dados ausentes

dftrain["category"].value_count()

dftrain = dftrain.dropna()


# Redefinindo o indice do dataframe

dftrain.groupby("category").count()

dftrain = dftrain.reset_index(drop=True)

# Atualização do dump
update_pickle("train", dftrain)

"""### 5.2 - Análise dos dados de teste"""

# Verifica a estrutura básica dos dados de teste

print('Shape of dataset ',dftest.shape)
print(dftest.columns)

dftest.head(5)

# Verificação das caracteristicas de cada coluna do arquivo

dftest.info()

# Verifica se exitem dados nulos no geral

if dftest.isnull().values.any():
    dftest[dftest.isnull().any(axis=1)] 
else:
   print("Nos dados de teste não existem dados nulos.")

# Remove dados ausentes

dftest = dftest.dropna()

# Redefinindo o indice do dataframe

dftest = dftest.reset_index(drop=True)
dftest.set_index('id', inplace=True)

# Atualização do dump
update_pickle("test", dftest)


"""### 5.3 - Limpeza / Tokenização / Conversão

#### 5.3.1 - Dados de Treino
"""

# Commented out IPython magic to ensure Python compatibility.
# # Limpeza
dftrain.apply(converter_text, axis=1)

# Atualização do dump
update_pickle("train", dftrain)

# Avaliação do resultado
dftrain.head()

"""#### 5.3.2 - Dados de Teste"""

# Commented out IPython magic to ensure Python compatibility.
# # Limpeza
dftest.apply(converter_text, axis=1)

# Atualização do dump
update_pickle("test", dftest)

# Avaliação do resultado
dftest.head()

"""## 6. Análise e Exploração dos Dados

### 6.1 Análise dos Dados
"""

# Verificação de como os textos ficaram após a limpeza:

print_plot(100)

dftrain[["title", "title_", "language", "language_"]].head(20)

"""## 7. Preparração dos dados para aplicação dos Modelos de Machine Learning"""

# Criando as variáveis para execução dos modelos

X_train = dftrain[["title_", "language_"]]
print('Shape of data tensor:', X_train.shape)

Y_train = dftrain["category_"]
print('Shape of label tensor:', Y_train.shape)

X_test = dftest[["title_", "language_"]]
print('Shape of data tensor:', X_test.shape)

"""## 8. Verificação do melhor modelo"""

### KNN

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import balanced_accuracy_score

neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train, Y_train)
y_pred = neigh.predict(X_test)

acc = balanced_accuracy_score(Y_train, y_pred)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(acc[0],acc[1]))

"""### 8.1 - LSTM"""

model = keras.layers.core.Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))
model.add(keras.layers.core.SpatialDropout1D(0.2))
model.add(keras.layers.core.LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(13, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,validation_split=0.1,callbacks=[keras.layers.core.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

accr = model.evaluate(X_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['acc'], label='train')
plt.plot(history.history['val_acc'], label='test')
plt.legend()
plt.show();

"""#### 8.1.1 - Simulação do modelo com os dados de teste"""

padded = dftest["title"].apply(token_text)
pred = model.predict(padded)

labels = dftrain['category'].value_counts()
print(pred, labels[np.argmax(pred)])

# ### 8.2 - CNN

# embedding_matrix = np.random.random((len(WORD_INDEX) + 1, EMBEDDING_DIM))

# for word, i in WORD_INDEX.items():
#     embedding_vector = embeddings_index.get(word)
#     if embedding_vector is not None:
#         # words not found in embedding index will be all-zeros.
#         embedding_matrix[i] = embedding_vector

# embedding_layer = Embedding(len(WORD_INDEX) + 1,
#                             EMBEDDING_DIM,weights=[embedding_matrix],
#                             input_length=MAX_SEQUENCE_LENGTH,trainable=True)

# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
# embedded_sequences = embedding_layer(sequence_input)
# l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)
# l_pool1 = MaxPooling1D(5)(l_cov1)
# l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)
# l_pool2 = MaxPooling1D(5)(l_cov2)
# l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)
# l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling
# l_flat = Flatten()(l_pool3)
# l_dense = Dense(128, activation='relu')(l_flat)
# preds = Dense(len(MAX_NB_WORDS), activation='softmax')(l_dense)

# model = Model(sequence_input, preds)
# model.compile(loss='categorical_crossentropy',
#               optimizer='rmsprop',
#               metrics=['acc'])

# print("Simplified convolutional neural network")
# model.summary()
# cp = ModelCheckpoint('model_cnn.hdf5', monitor='val_acc',verbose=1,save_best_only=True)

# history = model.fit(X_train, Y_train, validation_data=(X_val, y_val),epochs=EPOCHS, batch_size=BATCH_SIZE,callbacks=[cp])

# accr = model.evaluate(X_test)
# print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

# fig1 = plt.figure()
# plt.plot(history.history['loss'],'r',linewidth=3.0)
# plt.plot(history.history['val_loss'],'b',linewidth=3.0)
# plt.legend(['Training loss', 'Validation Loss'],fontsize=18)
# plt.xlabel('Epochs ',fontsize=16)
# plt.ylabel('Loss',fontsize=16)
# plt.title('Loss Curves :CNN',fontsize=16)
# fig1.savefig('loss_cnn.png')
# plt.show()

# fig2=plt.figure()
# plt.plot(history.history['acc'],'r',linewidth=3.0)
# plt.plot(history.history['val_acc'],'b',linewidth=3.0)
# plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)
# plt.xlabel('Epochs ',fontsize=16)
# plt.ylabel('Accuracy',fontsize=16)
# plt.title('Accuracy Curves : CNN',fontsize=16)
# fig2.savefig('accuracy_cnn.png')
# plt.show()

# """#### 8.2.1 - Simulação do modelo com os dados de teste"""

# padded = dftest["title"].apply(token_text)
# pred = model.predict(padded)

# labels = dftrain['category'].value_counts()
# print(pred, labels[np.argmax(pred)])

# ### 8.3 - RNN



# """#### 8.3.1 - Simulação do modelo com os dados de teste"""

# padded = dftest["title"].apply(token_text)
# pred = model.predict(padded)

# labels = dftrain['category'].value_counts()
# print(pred, labels[np.argmax(pred)])

# ### 8.4 - HAN

# """#### 8.4.1 - Simulação do modelo com os dados de teste"""

# padded = dftest["title"].apply(token_text)
# pred = model.predict(padded)

# labels = dftrain['category'].value_counts()
# print(pred, labels[np.argmax(pred)])

