{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install fastparquet --user\n",
    "# !pip install pyarrow --user\n",
    "# !pip install swifter --use\n",
    "# !pip install matplotlib --user\n",
    "# !pip install seaborn --user\n",
    "# !pip install joblib --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/train.pickle\n",
      "input/test.csv\n",
      "input/train.csv\n",
      "input/test.pickle\n",
      "input/sample_submission.csv\n",
      "input/train.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Carrega as bibliotecas de ambiente\n",
    "\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import re\n",
    "import glob\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "for dirname, _, filenames in os.walk('input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(700, 10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega as bibliotecas de ciências e gráficos\n",
    "\n",
    "import pickle\n",
    "\n",
    "import theano\n",
    "import nltk\n",
    "import swifter\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import fastparquet as fpq\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from fastparquet import ParquetFile\n",
    "from fastparquet import write\n",
    "\n",
    "from numba import vectorize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "gc.get_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importação das stopwords do pacote nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para os arquivos de dados\n",
    "PATH = \"input/\"\n",
    "\n",
    "# Regex\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "ONLY_STRING_WORD_RE = re.compile(r'\\w*\\d\\w*')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "LESS_THAN_WORD_RE = re.compile(r'\\b\\w{1,2,3}\\b')\n",
    "REMOVE_NUMBERS_RE = re.compile(' \\d+')\n",
    "\n",
    "# Stopwords\n",
    "STOPWORDS_0 = set(stopwords.words('english'))\n",
    "STOPWORDS_1 = set(stopwords.words('portuguese'))\n",
    "STOPWORDS_2 = set(stopwords.words('spanish'))\n",
    "\n",
    "# Número do bloco de leitura dos arquivos\n",
    "FILE_SIZE = 1000000\n",
    "# Número máximo de palavras usadas mais frequentes\n",
    "MAX_NB_WORDS = 50000\n",
    "# Numero máximo de palavras para saída\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "# Fixador.\n",
    "EMBEDDING_DIM = 100\n",
    "# Variável randomica\n",
    "RANDOM_STATE = 2011\n",
    "\n",
    "# Número de épocas\n",
    "EPOCHS = 15\n",
    "# Tamanho do bloco\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava Dataframe para Arquivos Parquet\n",
    "def write_parquet(df, file):\n",
    "    fparq = PATH + \"/files/\" + file + \".parquet\" \n",
    "    write(fparq, df, row_group_offsets=[0, 10000, 20000], compression='GZIP', file_scheme='hive')\n",
    "\n",
    "# Cria ou lê os dados pickle\n",
    "def file_pickle(file):\n",
    "    fpkl = PATH + file + \".pickle\"\n",
    "    fcsv = PATH + file + \".csv\"\n",
    "  \n",
    "    if os.path.isfile(fpkl):\n",
    "        df = pd.read_pickle(fpkl)\n",
    "    else:        \n",
    "        df = pd.read_csv(fcsv, header=0, sep=',', quotechar='\"', error_bad_lines=False, skipinitialspace=True)\n",
    "        df.to_pickle(fpkl)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Leitura de Arquivos Fracionados\n",
    "def file_chunk(file):    \n",
    "    for data in pd.read_csv(fcsv, chunksize=FILE_SIZE, header=0, sep=',', quotechar='\"', error_bad_lines=False, skipinitialspace=True):\n",
    "        print(data.shape)\n",
    "\n",
    "# Atualiza o arquivo pickle com novas informações\n",
    "def update_pickle(file, df):\n",
    "    fpkl = PATH + file + \".pkl\"\n",
    "    df.to_pickle(fpkl)\n",
    "    \n",
    "# Imprime os dados relativos ao indice passado\n",
    "def print_plot(index):\n",
    "    example = dftrain[dftrain.index == index][['title', 'category']].values[0]\n",
    "    \n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Category:', example[1])\n",
    "        \n",
    "# Limpeza dos dados: lower case; espaços do texto; caracteres especiais e simbolos; stop words e digitos\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = LESS_THAN_WORD_RE.sub('', text) # replace LESS_THAN_WORD symbols by space in text. substitute the matched string in LESS_THAN_WORD with space.\n",
    "    text = ONLY_STRING_WORD_RE.sub('', text) # replace ONLY_STRING_WORD_RE REMOVE words with numbers and letters in text. substitute the matched string in ONLY_STRING_WORD_RE with space.    \n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = REMOVE_NUMBERS_RE.sub('', text) # remove numbers which are in REMOVE_NUMBERS from text. substitute the matched string in REMOVE_NUMBERS with nothing. \n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS_0) # remove stopwors english from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS_1) # remove stopwors portugues from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS_2) # remove stopwors spanish from text\n",
    "    text = text.replace('\\d+', '')        \n",
    "    return text\n",
    "\n",
    "# Tokenização de textos\n",
    "def token_text(text):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters=string.punctuation, lower=True)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return sequences\n",
    "\n",
    "# Conversão de texto para variável categórica\n",
    "def dummie_text(text):\n",
    "    dummies = pd.get_dummies(text).values    \n",
    "    return dummies\n",
    "\n",
    "# Função principal de limpeza e tokenização (wording embedding)\n",
    "def converter_text(row):\n",
    "    title = clean_text(row[\"title\"])\n",
    "    row[\"title_\"] = token_text(title)\n",
    "    #row[\"language_\"] = dummie_text(row[\"language\"])\n",
    "  \n",
    "    if row.isin(['category']).any():\n",
    "        row[\"category_\"] = dummie_text(row[\"category\"])\n",
    "    row\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dftrain['titles'] = dftrain['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246955 entries, 0 to 246954\n",
      "Data columns (total 3 columns):\n",
      "id          246955 non-null int64\n",
      "title       246955 non-null object\n",
      "language    246955 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "dftest = file_pickle(\"test\")\n",
    "dftest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000000 entries, 0 to 19999999\n",
      "Data columns (total 4 columns):\n",
      "title            object\n",
      "label_quality    object\n",
      "language         object\n",
      "category         object\n",
      "dtypes: object(4)\n",
      "memory usage: 610.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dtype = {'title':int, 'label_quality':str, 'language':str, 'category':str}\n",
    "dftrain = file_pickle(\"train\")\n",
    "dftrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest[\"title_\"] = None\n",
    "dftest[\"language_\"] = None\n",
    "dftest.drop(columns=\"category_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "     \n",
    "results = Parallel(n_jobs=num_cores)(delayed(converter_text)(row) for row in dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3083a7b39f4138b805c53cc85cda38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=246955, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e19e08c3e40465e876b9d50ebf7b241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=246955, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Limpeza\n",
    "#dftest.apply(lambda row: converter_text(row), axis=1)\n",
    "%timeit\n",
    "dftest['title_'] = dftest['title'].swifter.apply(clean_text)\n",
    "gc.collect()\n",
    "\n",
    "dftest['title_'] = dftest['title_'].swifter.apply(token_text)\n",
    "gc.collect()\n",
    "\n",
    "dftest['language_'] = dftest['language'].swifter.apply(dummie_text)\n",
    "gc.collect()\n",
    "# Atualização do dump\n",
    "\n",
    "#update_pickle(\"test\", dftest)\n",
    "\n",
    "# Avaliação do resultado\n",
    "dftest.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Limpeza\n",
    "dftrain['title_'] = dftrain['title'].swifter.apply(clean_text)\n",
    "dftrain['title_'] = dftrain['title_'].swifter.apply(token_text)\n",
    "dftrain['language_'] = dftrain['language'].swifter.apply(dummie_text)\n",
    "dftrain['category_'] = dftrain['category'].swifter.apply(dummie_text)\n",
    "#dftrain.swifter.apply(lambda row: converter_text(row), axis=1)\n",
    "\n",
    "# Atualização do dump\n",
    "update_pickle(\"train\", dftrain)\n",
    "\n",
    "# Avaliação do resultado\n",
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
